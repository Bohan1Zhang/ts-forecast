{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d6ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\15173\\Desktop\\stock-ts-forecast\n",
      "Device: cpu\n",
      "Torch: 2.9.0+cpu\n",
      "[Step] Reading: C:\\Users\\15173\\Desktop\\stock-ts-forecast\\data\\processed\\features.csv\n",
      "[Step] CSV loaded. Shape: (3483, 24)\n",
      "[Step] Features: 22 | Total rows: 3483\n",
      "[Step] Any NaN in raw X?: True\n",
      "[Step] Any inf in raw X?: False\n",
      "[Step] Split idx: 2438 2960 3483\n",
      "[Step] Preprocess done. X_all_sc shape: (3483, 19)\n",
      "[Step] Seq sample counts — train/val/test: 2409 522 523\n",
      "[Step] DataLoaders ready.\n",
      "[Step] INPUT_DIM: 19\n",
      "\n",
      "[Step] === Training LSTM CLASSIFIER ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15173\\.conda\\envs\\stock-ts\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: [1 2 3]. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\15173\\.conda\\envs\\stock-ts\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: [1 2 3]. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.69365 | val_loss=0.69404\n",
      "Epoch 02 | train_loss=0.69213 | val_loss=0.69228\n",
      "Epoch 03 | train_loss=0.69187 | val_loss=0.69274\n",
      "Epoch 04 | train_loss=0.69106 | val_loss=0.69298\n",
      "Epoch 05 | train_loss=0.69115 | val_loss=0.69149\n",
      "Epoch 06 | train_loss=0.68980 | val_loss=0.68925\n",
      "Epoch 07 | train_loss=0.68871 | val_loss=0.68916\n",
      "Epoch 08 | train_loss=0.68799 | val_loss=0.68642\n",
      "Epoch 09 | train_loss=0.68533 | val_loss=0.68798\n",
      "Epoch 10 | train_loss=0.68462 | val_loss=0.69051\n",
      "Epoch 11 | train_loss=0.68068 | val_loss=0.68576\n",
      "Epoch 12 | train_loss=0.67999 | val_loss=0.69754\n",
      "Epoch 13 | train_loss=0.67523 | val_loss=0.68880\n",
      "Epoch 14 | train_loss=0.66988 | val_loss=0.70135\n",
      "Epoch 15 | train_loss=0.66912 | val_loss=0.71147\n",
      "Epoch 16 | train_loss=0.66314 | val_loss=0.69377\n",
      "[EarlyStop] No improvement for 5 epochs.\n",
      "[Saved]  c:\\Users\\15173\\Desktop\\stock-ts-forecast\\reports\\models\\lstm_clf.pt\n",
      "[TEST-CLS] acc=0.444 bacc=0.450 f1=0.426 prec=0.486 rec=0.379 auc=0.444\n",
      "\n",
      "[Step] === Training LSTM REGRESSOR ===\n",
      "Epoch 01 | train_loss=0.00051 | val_loss=0.00043\n",
      "Epoch 02 | train_loss=0.00035 | val_loss=0.00055\n",
      "Epoch 03 | train_loss=0.00035 | val_loss=0.00036\n",
      "Epoch 04 | train_loss=0.00034 | val_loss=0.00038\n",
      "Epoch 05 | train_loss=0.00033 | val_loss=0.00038\n",
      "Epoch 06 | train_loss=0.00034 | val_loss=0.00035\n",
      "Epoch 07 | train_loss=0.00033 | val_loss=0.00038\n",
      "Epoch 08 | train_loss=0.00032 | val_loss=0.00037\n",
      "Epoch 09 | train_loss=0.00032 | val_loss=0.00037\n",
      "Epoch 10 | train_loss=0.00032 | val_loss=0.00035\n",
      "Epoch 11 | train_loss=0.00032 | val_loss=0.00036\n",
      "[EarlyStop] No improvement for 5 epochs.\n",
      "[Saved]  c:\\Users\\15173\\Desktop\\stock-ts-forecast\\reports\\models\\lstm_reg.pt\n",
      "[TEST-REG] mae=0.012424 rmse=0.018149\n",
      "\n",
      " DONE. Outputs:\n",
      "  Models: c:\\Users\\15173\\Desktop\\stock-ts-forecast\\reports\\models\\lstm_clf.pt c:\\Users\\15173\\Desktop\\stock-ts-forecast\\reports\\models\\lstm_reg.pt\n",
      "  Figures saved under: C:\\Users\\15173\\Desktop\\stock-ts-forecast\\reports\\figures\\poster\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    precision_score, recall_score, roc_auc_score,\n",
    "    confusion_matrix, roc_curve, auc,\n",
    "    mean_absolute_error, mean_squared_error\n",
    ")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "try:\n",
    "    torch.set_num_interop_threads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "\n",
    "OUT_MODEL_DIR = PROJECT_ROOT / \"reports\" / \"models\"\n",
    "OUT_FIG_DIR   = PROJECT_ROOT / \"reports\" / \"figures\" / \"poster\"\n",
    "OUT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "FEATURES_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"features.csv\"\n",
    "print(\"[Step] Reading:\", FEATURES_PATH.resolve())\n",
    "\n",
    "if not FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find {FEATURES_PATH}. Please ensure data/processed/features.csv exists.\")\n",
    "\n",
    "df = pd.read_csv(FEATURES_PATH, index_col=0)\n",
    "print(\"[Step] CSV loaded. Shape:\", df.shape)\n",
    "\n",
    "\n",
    "try:\n",
    "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "except Exception:\n",
    "    pass\n",
    "df = df.sort_index()\n",
    "\n",
    "REQ = {\"next_ret_up\", \"next_log_ret\"}\n",
    "missing = REQ - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Missing required columns: {missing}. \"\n",
    "        f\"Expected at least {REQ}. Existing columns: {list(df.columns)[:10]} ...\"\n",
    "    )\n",
    "\n",
    "y_cls = df[\"next_ret_up\"].astype(np.float32).values      # 0/1 float\n",
    "y_reg = df[\"next_log_ret\"].astype(np.float32).values     # float\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [\"next_ret_up\", \"next_log_ret\"]]\n",
    "X_raw = df[feature_cols].astype(np.float32).values\n",
    "\n",
    "n, d = X_raw.shape\n",
    "print(\"[Step] Features:\", d, \"| Total rows:\", n)\n",
    "print(\"[Step] Any NaN in raw X?:\", np.isnan(X_raw).any())\n",
    "print(\"[Step] Any inf in raw X?:\", np.isinf(X_raw).any())\n",
    "\n",
    "\n",
    "WINDOW = 30\n",
    "TRAIN_END = int(n * 0.70)\n",
    "VAL_END   = int(n * 0.85)\n",
    "\n",
    "print(\"[Step] Split idx:\", TRAIN_END, VAL_END, n)\n",
    "\n",
    "if TRAIN_END <= WINDOW:\n",
    "    raise ValueError(f\"Train split too small for WINDOW={WINDOW}. \"\n",
    "                     f\"Need TRAIN_END > WINDOW, got TRAIN_END={TRAIN_END}.\")\n",
    "\n",
    "X_train_raw = X_raw[:TRAIN_END]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "X_train_imp = imputer.fit_transform(X_train_raw)\n",
    "X_train_sc  = scaler.fit_transform(X_train_imp)\n",
    "\n",
    "X_all_sc = scaler.transform(imputer.transform(X_raw)).astype(np.float32)\n",
    "print(\"[Step] Preprocess done. X_all_sc shape:\", X_all_sc.shape)\n",
    "\n",
    "all_t = np.arange(WINDOW - 1, n)\n",
    "t_train = all_t[all_t < TRAIN_END]\n",
    "t_val   = all_t[(all_t >= TRAIN_END) & (all_t < VAL_END)]\n",
    "t_test  = all_t[all_t >= VAL_END]\n",
    "print(\"[Step] Seq sample counts — train/val/test:\", len(t_train), len(t_val), len(t_test))\n",
    "\n",
    "if len(t_train) == 0 or len(t_val) == 0 or len(t_test) == 0:\n",
    "    raise ValueError(\"One of the splits has zero sequence samples. \"\n",
    "                     \"Try reducing WINDOW or check your dataset length/splits.\")\n",
    "\n",
    "\n",
    "class WindowDataset(Dataset):\n",
    "    def __init__(self, X_sc: np.ndarray, y: np.ndarray, t_indices: np.ndarray, window: int, task: str):\n",
    "        self.X = X_sc\n",
    "        self.y = y\n",
    "        self.t = t_indices\n",
    "        self.window = window\n",
    "        self.task = task  # \"cls\" or \"reg\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        t = int(self.t[i])\n",
    "        x = self.X[t - self.window + 1 : t + 1]  # (T, D)\n",
    "        if self.task == \"cls\":\n",
    "            y = self.y[t]\n",
    "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n",
    "        else:\n",
    "            y = self.y[t]\n",
    "            return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_cls_ds = WindowDataset(X_all_sc, y_cls, t_train, WINDOW, task=\"cls\")\n",
    "val_cls_ds   = WindowDataset(X_all_sc, y_cls, t_val,   WINDOW, task=\"cls\")\n",
    "test_cls_ds  = WindowDataset(X_all_sc, y_cls, t_test,  WINDOW, task=\"cls\")\n",
    "\n",
    "train_reg_ds = WindowDataset(X_all_sc, y_reg, t_train, WINDOW, task=\"reg\")\n",
    "val_reg_ds   = WindowDataset(X_all_sc, y_reg, t_val,   WINDOW, task=\"reg\")\n",
    "test_reg_ds  = WindowDataset(X_all_sc, y_reg, t_test,  WINDOW, task=\"reg\")\n",
    "\n",
    "train_cls_loader = DataLoader(train_cls_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_cls_loader   = DataLoader(val_cls_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_cls_loader  = DataLoader(test_cls_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_reg_loader = DataLoader(train_reg_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_reg_loader   = DataLoader(val_reg_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_reg_loader  = DataLoader(test_reg_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"[Step] DataLoaders ready.\")\n",
    "\n",
    "\n",
    "class LSTMBackbone(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, _) = self.lstm(x)\n",
    "        h_last = h_n[-1]\n",
    "        return h_last\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.backbone = LSTMBackbone(input_dim, hidden_dim, num_layers, dropout)\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        logits = self.head(h).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.backbone = LSTMBackbone(input_dim, hidden_dim, num_layers, dropout)\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        y = self.head(h).squeeze(-1)\n",
    "        return y\n",
    "\n",
    "INPUT_DIM = X_all_sc.shape[1]\n",
    "print(\"[Step] INPUT_DIM:\", INPUT_DIM)\n",
    "\n",
    "\n",
    "def train_with_early_stopping(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    max_epochs: int = 20,\n",
    "    patience: int = 5,\n",
    "    task: str = \"cls\"\n",
    "):\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        n_train = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.item() * xb.size(0)\n",
    "            n_train += xb.size(0)\n",
    "\n",
    "        train_loss = train_loss_sum / max(n_train, 1)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        n_val = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                val_loss_sum += loss.item() * xb.size(0)\n",
    "                n_val += xb.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / max(n_val, 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.5f} | val_loss={val_loss:.5f}\")\n",
    "\n",
    "        if val_loss < best_val - 1e-6:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"[EarlyStop] No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_loss_curve(history, title, out_path: Path):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history[\"train_loss\"], label=\"train\")\n",
    "    ax.plot(history[\"val_loss\"], label=\"val\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "print(\"\\n[Step] === Training LSTM CLASSIFIER ===\")\n",
    "cls_model = LSTMClassifier(INPUT_DIM, hidden_dim=64, num_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "cls_criterion = nn.BCEWithLogitsLoss()\n",
    "cls_optimizer = torch.optim.Adam(cls_model.parameters(), lr=1e-3)\n",
    "\n",
    "cls_hist = train_with_early_stopping(\n",
    "    cls_model, train_cls_loader, val_cls_loader,\n",
    "    cls_criterion, cls_optimizer,\n",
    "    max_epochs=20, patience=5, task=\"cls\"\n",
    ")\n",
    "\n",
    "plot_loss_curve(cls_hist, \"LSTM Classifier Loss\", OUT_FIG_DIR / \"lstm_cls_loss.png\")\n",
    "torch.save(cls_model.state_dict(), OUT_MODEL_DIR / \"lstm_clf.pt\")\n",
    "print(\"[Saved] \", OUT_MODEL_DIR / \"lstm_clf.pt\")\n",
    "\n",
    "cls_model.eval()\n",
    "all_probs = []\n",
    "all_y = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_cls_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = cls_model(xb).cpu()\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_probs.append(probs.numpy())\n",
    "        all_y.append(yb.numpy())\n",
    "\n",
    "y_true = np.concatenate(all_y).astype(int)\n",
    "y_prob = np.concatenate(all_probs)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "f1   = f1_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "try:\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "except Exception:\n",
    "    auc_score = float(\"nan\")\n",
    "\n",
    "print(f\"[TEST-CLS] acc={acc:.3f} bacc={bacc:.3f} f1={f1:.3f} prec={prec:.3f} rec={rec:.3f} auc={auc_score:.3f}\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm)\n",
    "ax.set_title(\"LSTM Confusion Matrix (TEST)\")\n",
    "ax.set_xlabel(\"Pred\")\n",
    "ax.set_ylabel(\"True\")\n",
    "for (i, j), v in np.ndenumerate(cm):\n",
    "    ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG_DIR / \"cm_lstm_test.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "\n",
    "try:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    ax.set_title(\"LSTM ROC (TEST)\")\n",
    "    ax.set_xlabel(\"FPR\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_FIG_DIR / \"roc_lstm_test.png\", dpi=200)\n",
    "    plt.close(fig)\n",
    "except Exception as e:\n",
    "    print(\"[Warn] ROC plot skipped:\", e)\n",
    "\n",
    "\n",
    "print(\"\\n[Step] === Training LSTM REGRESSOR ===\")\n",
    "reg_model = LSTMRegressor(INPUT_DIM, hidden_dim=64, num_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "reg_criterion = nn.MSELoss()\n",
    "reg_optimizer = torch.optim.Adam(reg_model.parameters(), lr=1e-3)\n",
    "\n",
    "reg_hist = train_with_early_stopping(\n",
    "    reg_model, train_reg_loader, val_reg_loader,\n",
    "    reg_criterion, reg_optimizer,\n",
    "    max_epochs=20, patience=5, task=\"reg\"\n",
    ")\n",
    "\n",
    "plot_loss_curve(reg_hist, \"LSTM Regressor Loss\", OUT_FIG_DIR / \"lstm_reg_loss.png\")\n",
    "torch.save(reg_model.state_dict(), OUT_MODEL_DIR / \"lstm_reg.pt\")\n",
    "print(\"[Saved] \", OUT_MODEL_DIR / \"lstm_reg.pt\")\n",
    "\n",
    "reg_model.eval()\n",
    "all_pred = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_reg_loader:\n",
    "        xb = xb.to(device)\n",
    "        pred = reg_model(xb).cpu().numpy()\n",
    "        all_pred.append(pred)\n",
    "        all_true.append(yb.numpy())\n",
    "\n",
    "y_true_r = np.concatenate(all_true).astype(np.float32)\n",
    "y_pred_r = np.concatenate(all_pred).astype(np.float32)\n",
    "\n",
    "mae  = mean_absolute_error(y_true_r, y_pred_r)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_true_r, y_pred_r)))\n",
    "print(f\"[TEST-REG] mae={mae:.6f} rmse={rmse:.6f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(y_true_r, label=\"True\", linewidth=1)\n",
    "ax.plot(y_pred_r, label=\"Pred\", linewidth=1)\n",
    "ax.set_title(\"Next-day log-return — LSTM (TEST)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG_DIR / \"lstm_reg_pred_vs_true_test.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "\n",
    "resid = y_true_r - y_pred_r\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(resid, bins=40)\n",
    "ax.set_title(\"Residuals — LSTM Regression (TEST)\")\n",
    "ax.set_xlabel(\"residual\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG_DIR / \"lstm_reg_residual_hist_test.png\", dpi=200)\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\n DONE. Outputs:\")\n",
    "print(\"  Models:\", (OUT_MODEL_DIR / \"lstm_clf.pt\"), (OUT_MODEL_DIR / \"lstm_reg.pt\"))\n",
    "print(\"  Figures saved under:\", OUT_FIG_DIR.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
